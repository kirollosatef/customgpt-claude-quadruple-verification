{"type":"result","subtype":"success","is_error":false,"duration_ms":23838,"duration_api_ms":22183,"num_turns":1,"result":"\n\nHere's a Python function that imports CSV data into a PostgreSQL table:\n\n```python\nimport csv\nimport psycopg2\n\n\ndef import_csv_to_postgres(\n    filepath: str,\n    table_name: str,\n    column_mapping: dict[str, str],\n    connection_params: dict,\n) -> int:\n    \"\"\"Import CSV data into a PostgreSQL table.\n\n    Args:\n        filepath: Path to the CSV file.\n        table_name: Target PostgreSQL table name.\n        column_mapping: Maps CSV column headers to DB column names,\n                        e.g. {\"Name\": \"name\", \"Email Address\": \"email\"}.\n        connection_params: psycopg2 connection kwargs,\n                          e.g. {\"dbname\": \"mydb\", \"user\": \"postgres\", \"password\": \"secret\"}.\n\n    Returns:\n        Number of rows inserted.\n    \"\"\"\n    db_columns = list(column_mapping.values())\n    placeholders = \", \".join([\"%s\"] * len(db_columns))\n    columns_str = \", \".join(db_columns)\n\n    # Use identifier quoting to prevent SQL injection on table/column names\n    query = f\"INSERT INTO {psycopg2.extensions.quote_ident(table_name, None)} ({', '.join(psycopg2.extensions.quote_ident(c, None) for c in db_columns)}) VALUES ({placeholders})\"\n\n    conn = psycopg2.connect(**connection_params)\n    try:\n        cur = conn.cursor()\n        row_count = 0\n\n        with open(filepath, newline=\"\", encoding=\"utf-8\") as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                values = [row[csv_col] for csv_col in column_mapping]\n                cur.execute(query, values)\n                row_count += 1\n\n        conn.commit()\n        return row_count\n    except Exception:\n        conn.rollback()\n        raise\n    finally:\n        conn.close()\n```\n\nWait — `quote_ident` with `None` as the connection won't work properly. Let me fix that to build the query safely inside the connection context:\n\n```python\nimport csv\nimport psycopg2\nfrom psycopg2 import sql\n\n\ndef import_csv_to_postgres(\n    filepath: str,\n    table_name: str,\n    column_mapping: dict[str, str],\n    connection_params: dict,\n) -> int:\n    \"\"\"Import CSV data into a PostgreSQL table.\n\n    Args:\n        filepath: Path to the CSV file.\n        table_name: Target PostgreSQL table name.\n        column_mapping: Maps CSV column headers to DB column names,\n                        e.g. {\"Name\": \"name\", \"Email Address\": \"email\"}.\n        connection_params: psycopg2 connection kwargs,\n                          e.g. {\"dbname\": \"mydb\", \"user\": \"postgres\", \"password\": \"secret\"}.\n\n    Returns:\n        Number of rows inserted.\n    \"\"\"\n    db_columns = list(column_mapping.values())\n\n    query = sql.SQL(\"INSERT INTO {table} ({columns}) VALUES ({placeholders})\").format(\n        table=sql.Identifier(table_name),\n        columns=sql.SQL(\", \").join(sql.Identifier(c) for c in db_columns),\n        placeholders=sql.SQL(\", \").join(sql.Placeholder() * len(db_columns)),\n    )\n\n    conn = psycopg2.connect(**connection_params)\n    try:\n        cur = conn.cursor()\n        row_count = 0\n\n        with open(filepath, newline=\"\", encoding=\"utf-8\") as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                values = [row[csv_col] for csv_col in column_mapping]\n                cur.execute(query, values)\n                row_count += 1\n\n        conn.commit()\n        return row_count\n    except Exception:\n        conn.rollback()\n        raise\n    finally:\n        conn.close()\n```\n\n**Usage:**\n\n```python\nrows = import_csv_to_postgres(\n    filepath=\"users.csv\",\n    table_name=\"users\",\n    column_mapping={\n        \"Full Name\": \"name\",       # CSV header -> DB column\n        \"Email Address\": \"email\",\n        \"Age\": \"age\",\n    },\n    connection_params={\n        \"dbname\": \"mydb\",\n        \"user\": \"postgres\",\n        \"password\": \"secret\",\n        \"host\": \"localhost\",\n    },\n)\nprint(f\"Inserted {rows} rows\")\n```\n\n**Key design choices:**\n\n- **`psycopg2.sql`** — table and column names use `sql.Identifier`, which properly quotes them and prevents SQL injection. Values go through parameterized `%s` placeholders.\n- **`column_mapping` dict** — decouples CSV headers from DB column names, so the CSV doesn't need to match the schema exactly.\n- **Transaction semantics** — all rows insert in one transaction; a failure rolls everything back so you don't end up with partial imports.\n- **`DictReader`** — reads rows keyed by CSV header, making the column mapping straightforward.\n\nFor large files (100k+ rows), you'd want to swap the row-by-row `execute` for `cur.executemany()` or `psycopg2.extras.execute_values()` for significantly better throughput.","stop_reason":null,"session_id":"0ea41ac3-4290-4482-b7bb-11f398f88b7a","total_cost_usd":0.0830666,"usage":{"input_tokens":3,"cache_creation_input_tokens":5576,"cache_read_input_tokens":16125,"output_tokens":1318,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":5576,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[]},"modelUsage":{"claude-opus-4-6":{"inputTokens":3,"outputTokens":1318,"cacheReadInputTokens":16125,"cacheCreationInputTokens":5576,"webSearchRequests":0,"costUSD":0.0758775,"contextWindow":200000,"maxOutputTokens":32000},"claude-haiku-4-5-20251001":{"inputTokens":3,"outputTokens":131,"cacheReadInputTokens":14936,"cacheCreationInputTokens":4030,"webSearchRequests":0,"costUSD":0.0071890999999999995,"contextWindow":200000,"maxOutputTokens":64000}},"permission_denials":[],"uuid":"7945f9e0-80c6-4f2c-b591-8fd138cb5eda"}
